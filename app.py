"""
Medical Domain Assistant — Hugging Face Spaces Deployment
=========================================================
Fine-tuned TinyLlama-1.1B-Chat on medalpaca/medical_meadow_medical_flashcards
using LoRA (r=16, alpha=32) via the PEFT library.

The base model weights are loaded from Hugging Face Hub and the LoRA adapter
weights are loaded from the ./medical_lora_adapter directory, which is
bundled with this Space.

Live demo: https://huggingface.co/spaces/denismitali/medical-assistant
"""

import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
ADAPTER    = "./medical_lora_adapter"
MAX_LEN    = 512

SYSTEM_PROMPT = (
    "You are a knowledgeable and helpful medical assistant. "
    "Answer medical questions accurately and concisely based on established "
    "medical knowledge. Always advise consulting a healthcare professional "
    "for personal medical decisions."
)


print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token    = tokenizer.eos_token
tokenizer.padding_side = "right"

print("Loading base model...")
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,   
    device_map="auto",
)

print("Applying LoRA adapter...")
model = PeftModel.from_pretrained(base_model, ADAPTER)
model.eval()

print("Model ready.")


def respond(question: str, temperature: float, max_tokens: int) -> str:
    """
    Generates a medical assistant response for a given question.

    Args:
        question    : The user's medical question
        temperature : Sampling temperature (0.1 = deterministic, 1.0 = creative)
        max_tokens  : Maximum number of new tokens to generate

    Returns:
        str: Model response with disclaimer appended
    """
    question = question.strip()
    if not question:
        return "Please enter a question."

    
    prompt = (
        f"<|system|>\n{SYSTEM_PROMPT}\n</s>\n"
        f"<|user|>\n{question}\n</s>\n"
        f"<|assistant|>\n"
    )

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_LEN,
    )

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=int(max_tokens),
            do_sample=True,
            temperature=float(temperature),
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
        )

    
    new_tokens = output_ids[0][inputs["input_ids"].shape[1]:]
    response   = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

    disclaimer = (
        "\n\n---\n"
        "Disclaimer: This response is generated by an AI model for educational "
        "purposes only. It does not constitute medical advice. Always consult a "
        "qualified healthcare professional for personal health decisions."
    )

    return response + disclaimer


demo = gr.Interface(
    fn=respond,
    inputs=[
        gr.Textbox(
            lines=3,
            label="Your Question",
            placeholder="Type a medical question here...",
        ),
        gr.Slider(
            minimum=0.1, maximum=1.0, value=0.3, step=0.05,
            label="Temperature",
            info="Lower = more deterministic and factual. Higher = more varied responses.",
        ),
        gr.Slider(
            minimum=50, maximum=400, value=200, step=25,
            label="Max New Tokens",
            info="Controls the maximum length of the response.",
        ),
    ],
    outputs=gr.Textbox(
        lines=12,
        label="Medical Assistant Response",
    ),
    title="Medical Domain Assistant",
    description=(
        "Fine-tuned TinyLlama-1.1B-Chat on Medical Flashcards via LoRA (PEFT).\n"
        "Trained on 3,000 examples from medalpaca/medical_meadow_medical_flashcards.\n"
        "For educational purposes only — not a substitute for professional medical advice."
    ),
    examples=[
        ["What is the mechanism of action of metformin?",              0.3, 200],
        ["What are the classic symptoms of appendicitis?",             0.3, 200],
        ["What is the difference between Type 1 and Type 2 diabetes?", 0.3, 200],
        ["How does heparin work as an anticoagulant?",                 0.3, 200],
        ["What are common side effects of ACE inhibitors?",            0.3, 200],
        ["Explain the renin-angiotensin-aldosterone system.",          0.3, 250],
        ["What is the Glasgow Coma Scale?",                            0.3, 200],
        ["How does aspirin inhibit platelet aggregation?",             0.3, 200],
    ],
    cache_examples=False,
    theme=gr.themes.Soft(),
)

if __name__ == "__main__":
    demo.launch()